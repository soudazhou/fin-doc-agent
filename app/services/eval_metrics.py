# =============================================================================
# Custom Evaluation Metrics — Finance-Specific Quality Measures
# =============================================================================
#
# Two custom metrics that complement DeepEval's built-in LLM-as-judge metrics.
# Both are pure Python — no LLM calls, no external dependencies, deterministic.
#
# 1. numerical_accuracy: Checks whether financial numbers in the answer match
#    expected values from the golden dataset. Critical for finance where a
#    hallucinated dollar amount or percentage is dangerous.
#
# 2. retrieval_recall_at_k: Measures whether the search agent retrieved the
#    correct source chunks. Standard IR metric adapted for our eval framework.
#
# DESIGN DECISION: These are standalone functions, not DeepEval metric
# subclasses. They don't need an LLM judge — they're deterministic
# computations. This keeps them fast, free, and testable without API keys.
# =============================================================================

from __future__ import annotations

import re
from dataclasses import dataclass


@dataclass
class MetricResult:
    """Result of evaluating a single custom metric."""

    score: float   # 0.0-1.0, higher is better
    reason: str    # Human-readable explanation


# ---------------------------------------------------------------------------
# Numerical Accuracy — Finance-Specific Metric
# ---------------------------------------------------------------------------


def numerical_accuracy(
    actual_output: str,
    expected_values: dict[str, str],
) -> MetricResult:
    """
    Measure whether financial numbers in the actual output match expected values.

    Checks each expected numerical value (dollar amounts, percentages, dates)
    against the actual output using normalised string matching.

    Args:
        actual_output: The answer generated by the RAG pipeline.
        expected_values: Dict of {label: expected_value} from the golden
            dataset's ``numerical_values`` field. Example:
            {"total_revenue": "$85.8 billion", "yoy_growth": "5%"}

    Returns:
        MetricResult with score = matched / total and per-value breakdown.
    """
    if not expected_values:
        return MetricResult(score=1.0, reason="No numerical values to check")

    normalized_output = _normalize_number(actual_output)

    matched = 0
    details: list[str] = []

    for label, expected in expected_values.items():
        normalized_expected = _normalize_number(expected)

        if normalized_expected in normalized_output:
            matched += 1
            details.append(f"{label}: MATCH ({expected})")
        else:
            # Try matching just the core number (without currency/units)
            core = _extract_core_number(expected)
            if core and core in normalized_output:
                matched += 1
                details.append(f"{label}: MATCH (fuzzy: {expected})")
            else:
                details.append(f"{label}: MISS (expected '{expected}')")

    score = matched / len(expected_values)
    reason = "; ".join(details)
    return MetricResult(score=score, reason=reason)


def _normalize_number(text: str) -> str:
    """
    Normalise a numeric string for fuzzy matching.

    Handles common financial formatting variations:
    - Comma separators: "1,234,567" → "1234567"
    - Scale words: "$85.8 billion" → "$85.8b"
    - Whitespace: "$ 85.8" → "$85.8"
    """
    text = text.lower().strip()
    text = text.replace(",", "")
    text = re.sub(r"\s+", " ", text)

    # Normalise scale words to abbreviations
    text = text.replace(" billion", "b").replace(" million", "m")
    text = text.replace(" trillion", "t")

    # Remove spaces between currency symbol and number
    text = re.sub(r"\$\s+", "$", text)

    return text


def _extract_core_number(text: str) -> str | None:
    """
    Extract just the numeric portion from a value string.

    "$85.8 billion" → "85.8"
    "5%" → "5"
    "$1.40" → "1.40"
    """
    match = re.search(r"[\d]+\.?[\d]*", text)
    return match.group(0) if match else None


# ---------------------------------------------------------------------------
# Retrieval Recall@k — Information Retrieval Metric
# ---------------------------------------------------------------------------


def retrieval_recall_at_k(
    retrieved_chunk_ids: list[int],
    expected_chunk_ids: list[int],
    k: int | None = None,
) -> MetricResult:
    """
    Measure whether expected source chunks appear in the retrieved results.

    Score = |expected ∩ retrieved[:k]| / |expected|

    This is a standard information retrieval metric adapted for our eval
    framework. It measures whether the agentic search found the right chunks.

    Args:
        retrieved_chunk_ids: Chunk IDs actually retrieved by the search agent.
        expected_chunk_ids: Chunk IDs tagged in the golden dataset as relevant.
        k: If set, only consider the top-k retrieved chunks. If None, consider
            all retrieved chunks.

    Returns:
        MetricResult with recall score and per-chunk breakdown.
    """
    if not expected_chunk_ids:
        return MetricResult(
            score=1.0,
            reason="No expected chunks specified (skipped)",
        )

    if k is not None:
        retrieved_set = set(retrieved_chunk_ids[:k])
    else:
        retrieved_set = set(retrieved_chunk_ids)

    expected_set = set(expected_chunk_ids)
    found = expected_set & retrieved_set
    missing = expected_set - retrieved_set

    score = len(found) / len(expected_set)
    reason = (
        f"Found {len(found)}/{len(expected_set)} expected chunks. "
        f"Missing: {sorted(missing) if missing else 'none'}"
    )
    return MetricResult(score=score, reason=reason)
